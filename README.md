# Speech Emotion Recognition
This study explores the domain of speech emotion recognition using the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset. With a focus on discerning emotions from recorded speech, our investigation involves a comprehensive analysis of the dataset, encompassing the extraction of relevant features from audio signals and the implementation of machine learning models. Leveraging classical machine learning and deep learning architectures, our approach aims to capture patterns in speech that signify diverse emotional states. The results demonstrate the effectiveness of different models in accurately classifying emotions, shedding light on the potential applications of Speech Emotion Recognition in various fields such as human-computer interaction, sentiment analysis, and mental health diagnostics. The findings also provide insights into the challenges and opportunities inherent in the RAVDESS dataset for advancing research in this evolving field.
